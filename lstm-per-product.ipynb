{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mas\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mplt\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m MinMaxScaler\n\u001b[1;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m Sequential\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m LSTM, Dense, Dropout\n\u001b[0;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcallbacks\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m EarlyStopping, ReduceLROnPlateau\n",
      "File \u001b[1;32md:\\Skripsi\\Model Steps\\Preprocessing\\Step 1\\.venv\\lib\\site-packages\\tensorflow\\__init__.py:49\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m tf2 \u001b[39mas\u001b[39;00m _tf2\n\u001b[0;32m     47\u001b[0m _tf2\u001b[39m.\u001b[39menable()\n\u001b[1;32m---> 49\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m __internal__\n\u001b[0;32m     50\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m __operators__\n\u001b[0;32m     51\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m audio\n",
      "File \u001b[1;32md:\\Skripsi\\Model Steps\\Preprocessing\\Step 1\\.venv\\lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39m\"\"\"Public API for tf._api.v2.__internal__ namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39msys\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mas\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m__internal__\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m autograph\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m__internal__\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m decorator\n\u001b[0;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m__internal__\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m dispatch\n",
      "File \u001b[1;32md:\\Skripsi\\Model Steps\\Preprocessing\\Step 1\\.venv\\lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\autograph\\__init__.py:9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39msys\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mas\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m_sys\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mautograph\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mag_ctx\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m control_status_ctx \u001b[39m# line: 34\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mautograph\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mimpl\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m tf_convert \u001b[39m# line: 493\u001b[39;00m\n",
      "File \u001b[1;32md:\\Skripsi\\Model Steps\\Preprocessing\\Step 1\\.venv\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:25\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtextwrap\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtraceback\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mautograph\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m operators\n\u001b[0;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mautograph\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m utils\n\u001b[0;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mautograph\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconverters\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m asserts\n",
      "File \u001b[1;32md:\\Skripsi\\Model Steps\\Preprocessing\\Step 1\\.venv\\lib\\site-packages\\tensorflow\\python\\autograph\\operators\\__init__.py:36\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39m\"\"\"This module implements operators that AutoGraph overloads.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[39mNote that \"operator\" is used loosely here, and includes control structures like\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39mconditionals and loops, implemented in functional form, using for example\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39mclosures for the body.\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[39m# Naming conventions:\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39m#  * operator names match the name usually used for the respective Python\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[39m#    idiom; examples: for_stmt, list_append\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[39m# subclasses namedtuple and contains any arguments that are only required\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[39m# for some specializations of the operator.\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mautograph\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moperators\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconditional_expressions\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m if_exp\n\u001b[0;32m     37\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mautograph\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moperators\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcontrol_flow\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m for_stmt\n\u001b[0;32m     38\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mautograph\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moperators\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcontrol_flow\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m if_stmt\n",
      "File \u001b[1;32md:\\Skripsi\\Model Steps\\Preprocessing\\Step 1\\.venv\\lib\\site-packages\\tensorflow\\python\\autograph\\operators\\conditional_expressions.py:18\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m\"\"\"Conditional expressions (e.g. the ternary if statement).\"\"\"\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mautograph\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moperators\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m control_flow\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mautograph\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m tensors\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m cond \u001b[39mas\u001b[39;00m tf_cond\n",
      "File \u001b[1;32md:\\Skripsi\\Model Steps\\Preprocessing\\Step 1\\.venv\\lib\\site-packages\\tensorflow\\python\\autograph\\operators\\control_flow.py:64\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtraceback\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mnumpy\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mas\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mnp\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mautograph\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moperators\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m py_builtins\n\u001b[0;32m     65\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mautograph\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moperators\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m variables\n\u001b[0;32m     66\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mautograph\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m ag_logging\n",
      "File \u001b[1;32md:\\Skripsi\\Model Steps\\Preprocessing\\Step 1\\.venv\\lib\\site-packages\\tensorflow\\python\\autograph\\operators\\py_builtins.py:29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m tensor_util\n\u001b[0;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m array_ops\n\u001b[1;32m---> 29\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m cond\n\u001b[0;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m control_flow_assert\n\u001b[0;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m gen_parsing_ops\n",
      "File \u001b[1;32md:\\Skripsi\\Model Steps\\Preprocessing\\Step 1\\.venv\\lib\\site-packages\\tensorflow\\python\\ops\\cond.py:25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m tensor_util\n\u001b[0;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m array_ops\n\u001b[1;32m---> 25\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m cond_v2\n\u001b[0;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m control_flow_ops\n\u001b[0;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m control_flow_util \u001b[39mas\u001b[39;00m util\n",
      "File \u001b[1;32md:\\Skripsi\\Model Steps\\Preprocessing\\Step 1\\.venv\\lib\\site-packages\\tensorflow\\python\\ops\\cond_v2.py:32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m dtypes\n\u001b[0;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m errors_impl\n\u001b[1;32m---> 32\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m func_graph \u001b[39mas\u001b[39;00m func_graph_module\n\u001b[0;32m     33\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m indexed_slices\n\u001b[0;32m     34\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m none_tensor  \u001b[39m# pylint: disable=unused-import\u001b[39;00m\n",
      "File \u001b[1;32md:\\Skripsi\\Model Steps\\Preprocessing\\Step 1\\.venv\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mweakref\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunction\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m trace_type\n\u001b[1;32m---> 22\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunction\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcapture\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m capture_container\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meager\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m context\n\u001b[0;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meager\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m execute\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:879\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:975\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1074\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "\n",
    "def calculate_rmspe(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the Root Mean Squared Percentage Error between true and predicted values.\n",
    "    \"\"\"\n",
    "    # Ensure there are no zero values in y_true to avoid division by zero\n",
    "    non_zero_mask = y_true != 0\n",
    "    y_true_safe = y_true[non_zero_mask]\n",
    "    y_pred_safe = y_pred[non_zero_mask]\n",
    "\n",
    "    if len(y_true_safe) == 0:\n",
    "        print(\"Warning: Cannot calculate RMSPE because all true values are zero.\")\n",
    "        return np.nan # Return Not-a-Number if calculation is impossible\n",
    "\n",
    "    # Calculate the percentage error\n",
    "    percentage_error = (y_true_safe - y_pred_safe) / y_true_safe\n",
    "    \n",
    "    # Square the percentage errors, take the mean, then the square root\n",
    "    rmspe = np.sqrt(np.mean(np.square(percentage_error))) * 100\n",
    "    \n",
    "    return rmspe\n",
    "\n",
    "# Configuration\n",
    "product_name = 'VIU Premium'\n",
    "# CRITICAL: Use a sequence length greater than 1 to capture patterns.\n",
    "SEQ_LEN = 1\n",
    "EPOCHS = 100 \n",
    "BATCH_SIZE = 32\n",
    "FUTURE_DAYS = 30\n",
    "VAL_SPLIT = 0.1\n",
    "train_test_split_ratio = 0.8\n",
    "\n",
    "# Feature columns\n",
    "features = [\n",
    "    'total_revenue',\n",
    "    'total_revenue_ema_10',\n",
    "    'total_revenue_lag_1'\n",
    "]\n",
    "\n",
    "# Load data from a single CSV\n",
    "try:\n",
    "    full_df = pd.read_csv('final_dataset.csv', parse_dates=['Tanggal'])\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'final_dataset.csv' not found. Please ensure the file is in the correct directory.\")\n",
    "    exit()\n",
    "\n",
    "# --- Feature Engineering ---\n",
    "product_df_base = full_df[full_df['Produk'] == product_name].copy()\n",
    "print(f\"Found {len(product_df_base)} rows for product: {product_name}\")\n",
    "\n",
    "if product_df_base.empty:\n",
    "    print(f\"No data found for product: {product_name} in 'final_dataset.csv'.\")\n",
    "    exit()\n",
    "\n",
    "product_df_base = product_df_base.sort_values('Tanggal').reset_index(drop=True)\n",
    "\n",
    "if 'total_revenue' not in product_df_base.columns:\n",
    "    print(\"Error: 'total_revenue' column is missing.\")\n",
    "    exit()\n",
    "\n",
    "product_df_base['total_revenue_ema_10'] = product_df_base['total_revenue'].ewm(span=10, adjust=False).mean()\n",
    "product_df_base['total_revenue_lag_1'] = product_df_base['total_revenue'].shift(1)\n",
    "\n",
    "# Handle potential NaNs\n",
    "product_df_cleaned = product_df_base.dropna(subset=features).reset_index(drop=True)\n",
    "\n",
    "if product_df_cleaned.empty:\n",
    "    print(f\"Error: No data remains for {product_name} after dropping NaNs.\")\n",
    "    exit()\n",
    "\n",
    "# Split data into training and testing sets\n",
    "split_index = int(len(product_df_cleaned) * train_test_split_ratio)\n",
    "train_df = product_df_cleaned.iloc[:split_index].copy()\n",
    "test_df = product_df_cleaned.iloc[split_index:].copy()\n",
    "\n",
    "if train_df.empty:\n",
    "    print(f\"Error: Training set is empty for product: {product_name}.\")\n",
    "    exit()\n",
    "    \n",
    "# Normalize features\n",
    "scaler = MinMaxScaler()\n",
    "train_scaled = scaler.fit_transform(train_df[features])\n",
    "test_scaled = scaler.transform(test_df[features])\n",
    "\n",
    "# Sequence generation\n",
    "def create_sequences(data, seq_len=SEQ_LEN):\n",
    "    x, y = [], []\n",
    "    if len(data) <= seq_len: \n",
    "        print(f\"Warning: Data length ({len(data)}) is not sufficient for sequence length ({seq_len}).\")\n",
    "        return np.array(x), np.array(y)\n",
    "    for i in range(seq_len, len(data)):\n",
    "        x.append(data[i-seq_len:i])\n",
    "        y.append(data[i][0])\n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "x_train, y_train = create_sequences(train_scaled)\n",
    "x_test, y_test = create_sequences(test_scaled)\n",
    "\n",
    "if x_train.shape[0] == 0:\n",
    "    print(f\"Error: Not enough data to create training sequences for {product_name} with SEQ_LEN={SEQ_LEN}.\")\n",
    "    exit()\n",
    "\n",
    "# Build model\n",
    "model = Sequential([\n",
    "    LSTM(\n",
    "        units=50, # A moderate number of units\n",
    "        activation='tanh',\n",
    "        input_shape=(SEQ_LEN, len(features)),\n",
    "        kernel_regularizer=l2(0.001) # L2 is often a good default\n",
    "    ),\n",
    "    Dropout(0.3),\n",
    "    Dense(units=25, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Optimizer\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae', RootMeanSquaredError(name='rmse')])\n",
    "model.summary()\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, verbose=1, min_lr=1e-6)\n",
    "callbacks_list = [early_stopping, reduce_lr]\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=VAL_SPLIT,\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title(f\"Training and Validation Loss - {product_name}\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "filename_base = product_name.replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "plt.savefig(f'./loss-plot/{filename_base}_loss_plot.png')\n",
    "plt.show()\n",
    "\n",
    "# --- Forecasting Logic ---\n",
    "# (Forecasting logic remains unchanged)\n",
    "if len(test_scaled) >= SEQ_LEN:\n",
    "    current_sequence_scaled = test_scaled[-SEQ_LEN:].copy()\n",
    "    history_df_unscaled = test_df[['Tanggal', 'Produk', 'total_revenue']].iloc[-SEQ_LEN-7:].copy()\n",
    "elif len(train_scaled) >= SEQ_LEN:\n",
    "    current_sequence_scaled = train_scaled[-SEQ_LEN:].copy()\n",
    "    history_df_unscaled = train_df[['Tanggal', 'Produk', 'total_revenue']].iloc[-SEQ_LEN-7:].copy()\n",
    "else:\n",
    "    print(\"Error: Not enough data to form an initial sequence for forecasting.\")\n",
    "    exit()\n",
    "\n",
    "predicted_revenues_unscaled_list = []\n",
    "for day_step in range(FUTURE_DAYS):\n",
    "    input_for_model = current_sequence_scaled.reshape(1, SEQ_LEN, len(features))\n",
    "    pred_scaled_revenue_component = model.predict(input_for_model, verbose=0)[0, 0]\n",
    "    dummy_scaled_row = np.zeros((1, len(features)))\n",
    "    dummy_scaled_row[0, 0] = pred_scaled_revenue_component\n",
    "    pred_unscaled_revenue = scaler.inverse_transform(dummy_scaled_row)[0, 0]\n",
    "    predicted_revenues_unscaled_list.append(pred_unscaled_revenue)\n",
    "    last_tanggal = history_df_unscaled['Tanggal'].iloc[-1]\n",
    "    next_tanggal = last_tanggal + pd.Timedelta(days=1)\n",
    "    new_row_dict = {'Tanggal': next_tanggal, 'Produk': product_name, 'total_revenue': pred_unscaled_revenue}\n",
    "    history_df_unscaled = pd.concat([history_df_unscaled, pd.DataFrame([new_row_dict])], ignore_index=True)\n",
    "    temp_df_for_feature_eng = history_df_unscaled.copy()\n",
    "    temp_df_for_feature_eng['total_revenue_ema_10'] = temp_df_for_feature_eng['total_revenue'].ewm(span=10, adjust=False).mean()\n",
    "    temp_df_for_feature_eng['total_revenue_lag_1'] = temp_df_for_feature_eng['total_revenue'].shift(1)\n",
    "    next_full_unscaled_feature_row_series = temp_df_for_feature_eng[features].iloc[-1]\n",
    "    next_full_unscaled_feature_row_filled = next_full_unscaled_feature_row_series.fillna(method='ffill').fillna(0)\n",
    "    next_full_scaled_feature_row = scaler.transform(next_full_unscaled_feature_row_filled.values.reshape(1, -1))\n",
    "    current_sequence_scaled = np.vstack((current_sequence_scaled[1:], next_full_scaled_feature_row))\n",
    "\n",
    "pred_rescaled = np.array(predicted_revenues_unscaled_list)\n",
    "\n",
    "# --- Actual vs Predicted Plotting and CSV Saving ---\n",
    "if x_test.shape[0] > 0 and len(y_test) > 0: \n",
    "    true_y_test_scaled = y_test.reshape(-1, 1)\n",
    "    padding_test = np.zeros((len(true_y_test_scaled), len(features) - 1)) \n",
    "    true_y_inverse = scaler.inverse_transform(np.hstack((true_y_test_scaled, padding_test)))[:, 0]\n",
    "\n",
    "    pred_test_scaled = model.predict(x_test, verbose=0) \n",
    "    pred_test_rescaled = scaler.inverse_transform(np.hstack((pred_test_scaled, np.zeros((len(pred_test_scaled), len(features) - 1)))))[:, 0]\n",
    "    \n",
    "    # Calculate and display RMSPE\n",
    "    rmspe_value = calculate_rmspe(true_y_inverse, pred_test_rescaled)\n",
    "    print(\"===============================================\")\n",
    "    print(f\"          Model Evaluation Results\")\n",
    "    print(\"-----------------------------------------------\")\n",
    "    print(f\"Product: {product_name}\")\n",
    "    print(f\"Root Mean Squared Percentage Error (RMSPE): {rmspe_value:.2f}%\")\n",
    "    print(\"===============================================\")\n",
    "    \n",
    "    # Determine the correct dates for the test results\n",
    "    actual_test_dates = test_df['Tanggal'].iloc[SEQ_LEN : SEQ_LEN + len(true_y_inverse)]\n",
    "\n",
    "    # --- SAVE TEST RESULTS TO CSV ---\n",
    "    if not actual_test_dates.empty:\n",
    "        # 1. Get the slice of the original test dataframe that corresponds to the predictions.\n",
    "        results_df = test_df.loc[actual_test_dates.index].copy()\n",
    "        \n",
    "        # 2. Add the predicted revenue as a new column.\n",
    "        results_df['predicted_revenue'] = pred_test_rescaled\n",
    "\n",
    "        # 3. For clarity, rename the original 'total_revenue' column to 'actual_revenue'.\n",
    "        results_df.rename(columns={'total_revenue': 'actual_revenue'}, inplace=True)\n",
    "        \n",
    "        # 4. Define the output filename and save the dataframe.\n",
    "        results_filename = f'./test-result/{filename_base}_predictions_vs_actuals.csv'\n",
    "        results_df.to_csv(results_filename, index=False)\n",
    "        \n",
    "        print(f\"Test results with predictions and features saved to: {results_filename}\")\n",
    "        print(\"===============================================\")\n",
    "    # --- END OF SAVE SECTION ---\n",
    "\n",
    "    # Plotting logic\n",
    "    if not actual_test_dates.empty:\n",
    "        last_actual_date = actual_test_dates.iloc[-1]\n",
    "        forecast_dates = pd.date_range(start=last_actual_date + pd.Timedelta(days=1), periods=FUTURE_DAYS)\n",
    "\n",
    "        plt.figure(figsize=(18, 9)) \n",
    "        plt.plot(actual_test_dates, true_y_inverse, label='Actual Revenue (Test)', color='black', marker='.', linestyle='-')\n",
    "        plt.plot(actual_test_dates, pred_test_rescaled, label='Predicted Revenue (Test)', color='blue', linestyle='--')\n",
    "        plt.plot(forecast_dates, pred_rescaled, label=f'Forecast (Next {FUTURE_DAYS} Days)', color='orange', linestyle='--')\n",
    "        \n",
    "        plt.title(f'{product_name} - Actual vs Predicted with {FUTURE_DAYS}-Day Forecast', fontsize=16)\n",
    "        plt.xlabel('Date', fontsize=12)\n",
    "        plt.ylabel('Revenue', fontsize=12)\n",
    "        plt.legend(fontsize=10)\n",
    "        plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'./test-plot/{filename_base}_test_and_forecast_plot.png')\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"x_test is empty. Cannot evaluate or save test results. Plotting forecast only.\")\n",
    "    # (Plotting forecast-only logic remains the same)\n",
    "\n",
    "# Save the trained model\n",
    "model.save(f'./model/lstm_model_{product_name}.h5')\n",
    "print(f\"Model saved to ./model/lstm_model_{product_name}.h5\")\n",
    "print(\"Script finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully.\n",
      "Initial DataFrame head:\n",
      "       Tanggal     Produk  total_revenue  total_revenue_ema_10  \\\n",
      "0  2024-01-02  IQIYI VIP       233244.0         181412.000000   \n",
      "1  2024-01-03  IQIYI VIP       310992.0         190836.000000   \n",
      "2  2024-01-04  IQIYI VIP       336908.0         212682.545455   \n",
      "3  2024-01-05  IQIYI VIP       142538.0         235268.991736   \n",
      "4  2024-01-06  IQIYI VIP       207328.0         218408.811420   \n",
      "\n",
      "   total_revenue_lag_1 revenue_class  \n",
      "0             181412.0  Bad for Sale  \n",
      "1             233244.0  Bad for Sale  \n",
      "2             310992.0  Bad for Sale  \n",
      "3             336908.0  Bad for Sale  \n",
      "4             142538.0  Bad for Sale  \n",
      "\n",
      "Initial DataFrame info:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1360 entries, 0 to 1359\n",
      "Data columns (total 6 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   Tanggal               1360 non-null   object \n",
      " 1   Produk                1360 non-null   object \n",
      " 2   total_revenue         1360 non-null   float64\n",
      " 3   total_revenue_ema_10  1360 non-null   float64\n",
      " 4   total_revenue_lag_1   1360 non-null   float64\n",
      " 5   revenue_class         1360 non-null   object \n",
      "dtypes: float64(3), object(3)\n",
      "memory usage: 63.9+ KB\n",
      "Error: 'Date' column not found. A 'Date' column is required for chronological splitting.\n",
      "Please ensure your dataset contains a date/timestamp column.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Date'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32md:\\Skripsi\\Model Steps\\Preprocessing\\Step 1\\.venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Date'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mPlease ensure your dataset contains a date/timestamp column.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     31\u001b[0m     exit()\n\u001b[1;32m---> 33\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mDate\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mto_datetime(df[\u001b[39m'\u001b[39;49m\u001b[39mDate\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m     34\u001b[0m df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39msort_values(by\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mDate\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39m# Sort by date and reset index\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mDataFrame sorted by Date.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Skripsi\\Model Steps\\Preprocessing\\Step 1\\.venv\\lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   4103\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32md:\\Skripsi\\Model Steps\\Preprocessing\\Step 1\\.venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(casted_key, \u001b[39mslice\u001b[39m) \u001b[39mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[39misinstance\u001b[39m(casted_key, abc\u001b[39m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m(\u001b[39misinstance\u001b[39m(x, \u001b[39mslice\u001b[39m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[39mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Date'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split # Still import, but we'll use manually for chronological\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, auc\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
    "import numpy as np\n",
    "\n",
    "# --- Plotting Libraries ---\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set a style for plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# --- 1. Load the Dataset ---\n",
    "try:\n",
    "    df = pd.read_csv('final_dataset.csv')\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "    print(\"Initial DataFrame head:\\n\", df.head())\n",
    "    print(\"\\nInitial DataFrame info:\\n\")\n",
    "    df.info()\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: final_dataset.csv not found. Please make sure the file is in the correct directory.\")\n",
    "    exit()\n",
    "\n",
    "# --- Preprocessing for Time-Series ---\n",
    "# Ensure a 'Date' column exists and is in datetime format for chronological splitting\n",
    "if 'Tanggal' not in df.columns:\n",
    "    print(\"Error: 'Tanggal' column not found. A 'Tanggal' column is required for chronological splitting.\")\n",
    "    print(\"Please ensure your dataset contains a Tanggal/timestamp column.\")\n",
    "    exit()\n",
    "\n",
    "df['Tanggal'] = pd.to_datetime(df['Tanggal'])\n",
    "df = df.sort_values(by='Tanggal').reset_index(drop=True) # Sort by Tanggal and reset index\n",
    "print(\"\\nDataFrame sorted by Date.\")\n",
    "\n",
    "\n",
    "# --- 2. Verify and Encode the 'revenue_class' Target Variable ---\n",
    "if 'revenue_class' not in df.columns:\n",
    "    print(\"Error: 'revenue_class' column not found in the dataset.\")\n",
    "    print(\"Please ensure your preprocessing code that creates 'revenue_class' has been run and saved to 'final_dataset.csv'.\")\n",
    "    exit()\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df['revenue_class_encoded'] = label_encoder.fit_transform(df['revenue_class'])\n",
    "\n",
    "target_names = label_encoder.classes_\n",
    "num_classes = len(target_names)\n",
    "print(f\"\\nLabel mapping: {list(zip(label_encoder.classes_, range(len(label_encoder.classes_))))}\")\n",
    "\n",
    "\n",
    "# --- 3. Define Features (X) and Target (y) ---\n",
    "features = [\n",
    "    'total_revenue',\n",
    "    # 'total_revenue_ema_7',\n",
    "    # 'total_revenue_ema_10',\n",
    "    # 'total_revenue_ema_14',\n",
    "    # 'total_revenue_lag_1'\n",
    "]\n",
    "\n",
    "missing_features = [f for f in features if f not in df.columns]\n",
    "if missing_features:\n",
    "    print(f\"\\nError: The following specified features are missing from your dataset: {missing_features}\")\n",
    "    print(\"Please check your 'final_dataset.csv' columns.\")\n",
    "    exit()\n",
    "\n",
    "X = df[features]\n",
    "y = df['revenue_class_encoded']\n",
    "\n",
    "# Handle NaN values after feature creation (e.g., from lags/EMAs)\n",
    "# For time-series, dropping NaNs from the beginning is often the most direct approach.\n",
    "# For 400 rows, losing a few rows is acceptable if it covers the NaN-generating period.\n",
    "initial_rows_before_nan_drop = X.shape[0]\n",
    "df_cleaned = pd.concat([X, y], axis=1).dropna() # Combine X and y to drop corresponding rows\n",
    "X = df_cleaned[features]\n",
    "y = df_cleaned['revenue_class_encoded']\n",
    "rows_dropped = initial_rows_before_nan_drop - X.shape[0]\n",
    "\n",
    "if rows_dropped > 0:\n",
    "    print(f\"\\nWarning: {rows_dropped} rows dropped due to NaN values (likely from lag/EMA features).\")\n",
    "    print(f\"Remaining dataset size after NaN handling: {X.shape[0]} rows.\")\n",
    "\n",
    "\n",
    "# --- 4. Chronological Split Data into Training and Testing Sets ---\n",
    "# For a 400-row dataset, a common split could be 70-80% for training, 20-30% for testing.\n",
    "# Let's aim for 80% training, 20% testing.\n",
    "train_size = int(len(X) * 0.8) # 80% for training\n",
    "print(f\"\\nTotal data points after NaN handling: {len(X)}\")\n",
    "print(f\"Training set size: {train_size} rows\")\n",
    "print(f\"Test set size: {len(X) - train_size} rows\")\n",
    "\n",
    "\n",
    "X_train, X_test = X.iloc[:train_size], X.iloc[train_size:]\n",
    "y_train, y_test = y.iloc[:train_size], y.iloc[train_size:]\n",
    "\n",
    "print(f\"\\nTraining set shape: {X_train.shape}, Test set shape: {X_test.shape}\")\n",
    "print(f\"Training target distribution:\\n{y_train.value_counts(normalize=True)}\")\n",
    "print(f\"Test target distribution:\\n{y_test.value_counts(normalize=True)}\")\n",
    "\n",
    "\n",
    "# --- 5. Train the XGBoost Classifier ---\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    objective='multi:softmax',\n",
    "    num_class=num_classes,\n",
    "    eval_metric='mlogloss',\n",
    "    n_estimators=300,        # You might consider fewer estimators for smaller datasets\n",
    "    learning_rate=0.05,\n",
    "    max_depth=4,             # Keep max_depth relatively low to prevent overfitting\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    use_label_encoder=False,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"\\nTraining XGBoost model...\")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "print(\"XGBoost model training complete.\")\n",
    "\n",
    "# --- 6. Make Predictions and Evaluate the Model ---\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "y_pred_proba = xgb_model.predict_proba(X_test) # Get probabilities for ROC curve\n",
    "\n",
    "print(\"\\n--- Model Evaluation ---\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=target_names))\n",
    "\n",
    "\n",
    "# --- 7. Plotting Model Performance ---\n",
    "\n",
    "# --- Plot 1: Confusion Matrix Heatmap ---\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=target_names, yticklabels=target_names)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n",
    "# --- Plot 2: One-vs-Rest ROC Curve ---\n",
    "# Binarize the output for multi-class ROC\n",
    "y_test_bin = label_binarize(y_test, classes=range(num_classes))\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(num_classes):\n",
    "    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_pred_proba[:, i])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, lw=2,\n",
    "             label=f'ROC curve for {target_names[i]} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('One-vs-Rest ROC Curves for Sales Quality Classification')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- Plot 3: Feature Importance ---\n",
    "feature_importances = pd.Series(xgb_model.feature_importances_, index=X.columns)\n",
    "feature_importances_sorted = feature_importances.sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=feature_importances_sorted.values, y=feature_importances_sorted.index, palette='viridis')\n",
    "plt.title('XGBoost Feature Importance')\n",
    "plt.xlabel('Importance (F-score)')\n",
    "plt.ylabel('Features')\n",
    "plt.tight_layout() # Adjust layout to prevent labels from overlapping\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
